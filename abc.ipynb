{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Intro to Bayesian Nets example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Creating the synthetic applicant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import erf, sqrt, log\n",
    "import numpy as np\n",
    "from causalnex.structure import StructureModel\n",
    "from IPython.display import Image\n",
    "from causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE\n",
    "from causalnex.structure.notears import from_pandas\n",
    "from causalnex.network import BayesianNetwork\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from causalnex.evaluation import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from causalnex.evaluation import roc_auc\n",
    "from causalnex.inference import InferenceEngine\n",
    "from causalnex.discretiser import Discretiser\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create 1000 candidates with normal distributed drive:\n",
    "size = 6000\n",
    "cols = ['drive']\n",
    "candidate = pd.DataFrame((((np.random.normal(loc=4, size=(size,len(cols)), scale=1))/8)*100), columns=cols, index=[*range(size)])\n",
    "\n",
    "\n",
    "def normal_cdf_mapper(banana,sigma=1,mu=4):\n",
    "    \"\"\"\n",
    "    cumulative distrib fct of the normal distribution for the calculation of social_commitment/education\n",
    "    \"\"\"\n",
    "    x = banana['drive']*8/100\n",
    "    return int((1 + erf((x-mu)/(sigma*sqrt(2))))/2 *1.3)\n",
    "\n",
    "def normal_cdf(x,sigma=1,mu=4):\n",
    "    \"\"\"\n",
    "    cumulative distrib fct of the normal distribution for the calculation of social_commitment/education\n",
    "    \"\"\"\n",
    "    x = x * 8/100\n",
    "    return int((1 + erf((x-mu)/(sigma*sqrt(2))))/2 *100)\n",
    "\n",
    "# Introduce skewed distributions for font:\n",
    "candidate['font'] = np.random.choice(np.arange(0, 4), size=size, p=[0.50, 0.3, 0.1, 0.1])\n",
    "\n",
    "# And for soc_comm/edu shifted log/normal distributions:\n",
    "candidate['social_commitment'] = candidate['drive']/4 + np.random.lognormal(size=size, mean = 4, sigma=.2)\n",
    "candidate['social_commitment'] = (candidate['social_commitment']/candidate['social_commitment'].max())*100\n",
    "candidate['education'] = candidate['drive']/4 + ((np.random.normal(loc=4, size=size, scale=1))/8)*100\n",
    "candidate['education'] = (candidate['education']/candidate['education'].max())*100\n",
    "\n",
    "# Creating a causal dependence between drive/education and drive/work experience\n",
    "def drive_mapper(banana, interval, offset):\n",
    "    \"\"\"\n",
    "    Function encoding the relation between drive/education & drive/work experience.\n",
    "    \"\"\"\n",
    "    if banana['drive']>=interval[0]:\n",
    "        return normal_cdf(banana['drive']) + offset\n",
    "    if banana['drive']>=interval[1]:\n",
    "        return normal_cdf(banana['drive'])* (((np.random.normal(size=1,scale=10)+6)/12)*10)[0]\n",
    "    if banana['drive']>=interval[2]:\n",
    "        return (((np.random.normal(size=1,scale=10)+6)/12)*10+50)[0]\n",
    "    if banana['drive']>=0:\n",
    "        return (((np.random.normal(size=1,scale=10)+6)/12)*10+30)[0]\n",
    "    return 0 \n",
    "\n",
    "# Make it more likely to be hired if your colour is 1, with a likelihood of getting hired of 60% if colour==1, and 50% if colour==0:\n",
    "def hired_mapper(banana):\n",
    "    \"\"\"\n",
    "    Function making it more likely to be hired if colour==1 then colour==0, at comparable skill/experience/... levels\n",
    "    \"\"\"\n",
    "    # if banana['font']==1 and banana['hired']>=0.6* (banana['hired'].max()):\n",
    "    #     return np.random.choice(np.arange(0, 3), p=[0.1, 0.2, 0.7])\n",
    "    # if banana['font']==1 and banana['hired']<0.6* (banana['hired'].max()):\n",
    "    #     return np.random.choice(np.arange(0, 3), p=[0.1, 0.2, 0.7])\n",
    "    # if banana['font']==0 and banana['hired']>=0.75* (banana['hired'].max()):\n",
    "    #     return np.random.choice(np.arange(0, 3), p=[0.1, 0.2, 0.7])\n",
    "    # if banana['font']==0 and banana['hired']<0.75* (banana['hired'].max()):\n",
    "    #     return np.random.choice(np.arange(0, 3), p=[0.1, 0.2, 0.7])\n",
    "    # return 0 \n",
    "    if banana['font']>=3 and banana['hired']>=0.2*banana['hired'].max():\n",
    "        return 1\n",
    "    if banana['font']>=2 and banana['hired']>=0.5*banana['hired'].max():\n",
    "        return 0\n",
    "    if banana['font']>=1 and banana['hired']>=0.8*banana['hired'].max():\n",
    "        return 1\n",
    "    if banana['font']==0 and banana['hired']>=0.9*banana['hired'].max():\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "# # Completeing the dataset with the encoded data:\n",
    "# candidate['education'] = candidate.apply(lambda x: drive_mapper(x, [80, 65, 50],0), axis=1)\n",
    "candidate['work_experience'] = candidate.apply(lambda x: drive_mapper(x, [50, 35, 20],0), axis=1)\n",
    "\n",
    "# More higher skill levels/experience/... are desirable, so to make the likelihood of getting hired depend on it:\n",
    "# candidate['hired'] = candidate.sum(axis=1)\n",
    "candidate['hired'] = 1*candidate['font'] + 4*candidate['education'] + 2.5*candidate['work_experience'] + 2.5*candidate['drive'] + 4*candidate['social_commitment']+ 4*candidate['drive']\n",
    "# print(candidate['hired'].describe())\n",
    "candidate['hired'] = candidate.apply(lambda x: hired_mapper(x), axis=1)\n",
    "\n",
    "# and loading the initial data:\n",
    "abc = pd.read_csv(r'observations_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the code for the initial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cpd p(c|h)\n",
    "# coditional probability table of cooking given hungry\n",
    "# reality check: each column should add up to 1\n",
    "cpd2 = pd.pivot_table(abc[[hungry,cooking]],index=hungry, columns=cooking, aggfunc=np.size); \n",
    "cpd2 = (cpd2/cpd2.sum()); cpd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpd p(h)\n",
    "cpd3 = pd.DataFrame(abc.hungry.value_counts(), columns=[hungry])\n",
    "cpd3 = cpd3/cpd3.sum(); cpd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpd p(lt|c,h)\n",
    "cpd1 = pd.pivot_table(abc,index=lunchtime, columns=[cooking,hungry], aggfunc=np.size)\n",
    "cpd1 = cpd1/cpd1.sum(); cpd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the structure models, which will contain the graphs:\n",
    "sm = StructureModel()\n",
    "sm2 = StructureModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a cyclical graph:\n",
    "sm.add_edges_from([\n",
    "    ('hungry', 'cooking'),\n",
    "    ('cooking', 'hungry'),\n",
    "    ('cooking', 'lunch time'),\n",
    "    ('hungry', 'lunch time'),\n",
    "    ('lunch time', 'cooking'),\n",
    "    ('lunch time', 'hungry')\n",
    "])\n",
    "\n",
    "# example of a directed acyclical graph\n",
    "sm2.add_edges_from([\n",
    "    ('hungry', 'cooking'),\n",
    "    ('cooking', 'lunch time'),\n",
    "    ('hungry', 'lunch time')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = plot_structure(\n",
    "    sm)\n",
    "filename = \"./plot1.png\"\n",
    "viz.draw(filename)\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = plot_structure(\n",
    "    sm2)\n",
    "filename = \"./plot2.png\"\n",
    "viz.draw(filename)\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the 'True' and 'False' observations to 1 and 0:\n",
    "maps = {True: 1, False: 0}\n",
    "abc2 = abc.copy()\n",
    "for _ in abc2.columns:\n",
    "    abc2[_] = abc2[_].map(maps)\n",
    "abc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability for p(lt|h,c)\\*p(c|h)\\*p(h) with h=True, c=False, lt=False\n",
    "p_c_lt_h = cpd1.iloc[0,0]*cpd2.iloc[0,1]*cpd3.iloc[0,0]\n",
    "print(f\"Probability for p(h=True, c=False, lt=False) = {p_c_lt_h*100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And the hiring data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check histograms for problems:\n",
    "_ = candidate.hist(bins=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some further very light EDA (exploratory data analysis):\n",
    "cor = candidate.corr()\n",
    "\n",
    "# plot the heatmap:\n",
    "_ = sns.heatmap(cor,\n",
    "                xticklabels=cor.columns,\n",
    "                yticklabels=cor.columns,\n",
    "                annot=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "# lovely, as expected from the dataset creation, we see the right correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph, and infer nodes and edges from pandas file:\n",
    "sm5 = StructureModel()\n",
    "sm5.add_edge('education','hired')\n",
    "sm5.add_edge('font','hired')\n",
    "sm5.add_edge('drive','education')\n",
    "sm5.add_edge('drive','hired')\n",
    "sm5.add_edge('drive','work_experience')\n",
    "sm5.add_edge('drive','social_commitment')\n",
    "sm5.add_edge('social_commitment', 'hired')\n",
    "sm5.add_edge('education','work_experience')\n",
    "sm5.add_edge('work_experience', 'hired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = plot_structure(\n",
    "    sm5,\n",
    "    graph_attributes={\"scale\": \"1.3\"},\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK)\n",
    "filename = \"./structure_model1.png\"\n",
    "viz.draw(filename)\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a continuous range of data for most variables. As we are not interested in the differences between e.g. education=34 and education=35, we split the variables and discretise them into fice levels: very low (0) to very high(4):\n",
    "\n",
    "def intervals(df, col):\n",
    "    \"\"\"\n",
    "    Create intervals for discretisation of df columns based on quartiles.\n",
    "    \"\"\"\n",
    "    max_, min_ = df[col].max(), df[col].min()\n",
    "    interval = [0.2*(max_-min_)+min_,0.4*(max_-min_)+min_,0.6*(max_-min_)+min_,0.8*(max_-min_)+min_]\n",
    "    a = Discretiser(method=\"fixed\", numeric_split_points=interval).transform(df[col].values)\n",
    "    return a\n",
    "\n",
    "candidate[\"education\"] = intervals(candidate, 'education')\n",
    "candidate[\"work_experience\"] = intervals(candidate, 'work_experience')\n",
    "candidate[\"drive\"] = intervals(candidate, 'drive')\n",
    "candidate[\"social_commitment\"] = intervals(candidate, 'social_commitment')\n",
    "\n",
    "# if there is a limited amount of data, a discretisation like the above will dramatically decrease performance by masking the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the discretisation - are all levels well populated:\n",
    "a = candidate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map integers back to legible labels:\n",
    "\n",
    "# label_map = {0: \"v low\", 1: \"low\", 2: \"high\", 3: \"v high\"}\n",
    "# cols = ['volatile_acidity','citric_acid','density','fixed_acidity','total_sulfur_dioxide','free_sulfur_dioxide','residual_sugar','chlorides','pH','sulphates','alcohol']\n",
    "\n",
    "# for _ in cols:\n",
    "#     wine[_] = wine[_].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split the data:\n",
    "train_w, test_w = train_test_split(candidate, train_size=0.95, test_size=0.05, random_state=42)\n",
    "\n",
    "bn = BayesianNetwork(sm5)\n",
    "\n",
    "# Ensure that all data is seen, as some of the data may only be in the test set:\n",
    "bn = bn.fit_node_states(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain conditional probabilities from the train data:\n",
    "bn = bn.fit_cpds(train_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # conditional probabilities for quality:\n",
    "# bn.cpds['hired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's check our bayesian nets prediction for the test_candidate:\n",
    "test_candidate = test_w.index[1:2][0]\n",
    "prediction = bn.predict_probability(test_w.iloc[1:2,:],'hired')\n",
    "pred = (prediction.loc[test_candidate,prediction.idxmax(axis=1)].index[0]).lstrip('hired'+\"_\")\n",
    "truth = test_w.loc[test_candidate,'hired']\n",
    "print(f'The prediction is \"{\"hired\" if float(pred)>=0.5 else \"not hired\"}\", the true quality value is \"{\"not hired\" if truth==0 else \"not hired\"}\".','\\n\\n')\n",
    "print(\"And a predictions outputs probabilities looking like this:\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To do some quality measurements, lets calculate reciever operator curve (ROC) and area under curve (AUC):\n",
    "\n",
    "# prepare the truth: \n",
    "truth = pd.get_dummies(test_w['hired'])\n",
    "for var in bn.node_states['hired']:\n",
    "        if var not in truth.columns:\n",
    "            truth[var] = 0\n",
    "\n",
    "predictions = bn.predict_probability(test_w,'hired')\n",
    "predictions.columns = predictions.columns.str.replace('hired_','')\n",
    "fpr, tpr, _= metrics.roc_curve(truth.values.ravel(), predictions.values.ravel()) \n",
    "roc = list(zip(fpr, tpr))\n",
    "a = pd.DataFrame(roc, columns=['FPR','TPR'])\n",
    "a['auc'] = a['TPR']*(1-a['FPR'])\n",
    "max_ =  a['auc'].idxmax()\n",
    "\n",
    "print(f'Area under the curve (AUC): {a.loc[max_,\"auc\"]*100:.1f}%')\n",
    "# The area under curve (AUC) is a good measure of model quality, in the case of a balanced dataset. A perfect model would have a AUC=1, a model deciding randomly AUC=1/2, and if AUC<1/2 you probably have a sign error somewhere, or the model is really, terribly bad.\n",
    "\n",
    "# classification_report(bn, test_w, \"hired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reciever operator curve (ROC), from which the AUC is determined. The AUC corresponds to the FPR*TPR maximum on the ROC curve inflection point.\n",
    "_ = plt.plot(a['FPR'],a['TPR'], label='ROC Curve')\n",
    "_ = plt.plot([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],label='Random Choice')\n",
    "_ = plt.scatter(a.loc[max_,'FPR'], a.loc[max_,'TPR'], c='r', label='AUC')\n",
    "_ = plt.legend(loc='lower right')\n",
    "# _ = plt.grid()\n",
    "_ = plt.xlabel('False Positive Rate')\n",
    "_ = plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the quality of the model, where in an ideal case the bleu line would always very close the ideal curve fulfilling TPR*FPR=1.\n",
    "The >86% area under the curve (AUC) is a good result, which with some work can surely be improved upon, especially considering the low values for [TPR](https://en.wikipedia.org/wiki/True_Positive_Rate).<br>\n",
    "(Caveat: the ROC-AUC has to be taken cum granulo salis, as there is an imbalance between hired/not hired, which also manifests in the FPR/TPR skew.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of Counterfactuals and Interventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-fit to include all data in dataset for node modification calcs:\n",
    "\n",
    "bn = bn.fit_cpds(candidate, method=\"BayesianEstimator\", bayes_prior=\"K2\")\n",
    "ie = InferenceEngine(bn)\n",
    "\n",
    "# marginals = ie.query()\n",
    "# marginals[\"quality\"]\n",
    "# marginals_df = pd.DataFrame(columns=['probability'])\n",
    "# for _ in marginals[\"quality\"].keys():\n",
    "#     marginals_df.loc[_,'probability'] = marginals[\"quality\"][_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_of_changes(change_node, result_node, new_distrib, ie_):\n",
    "    \"\"\"\n",
    "    Visualise the effects of changes in one node\n",
    "\n",
    "    Input:\n",
    "        change_node: (str) node the change is applied to\n",
    "        result_node: (str) node we are interested in\n",
    "        new_distrib: (dict) new distribution of values with keys according\n",
    "                            to result node\n",
    "        ie_: (causal nex inference engine object) \n",
    "\n",
    "    Returns:\n",
    "        plot: Bar plot of the resulting changes\n",
    "    \"\"\"\n",
    "    marg_list = [ie_.query({change_node: _})['hired'] for _ in ie._cpds[change_node]]\n",
    "    before = ie_.query()[result_node]\n",
    "    ie_.do_intervention(change_node,new_distrib)\n",
    "    after = ie.query()[result_node]\n",
    "    ie_.reset_do(change_node)\n",
    "    changes = pd.DataFrame([before,after],\n",
    "                           index=[f'old {change_node}',\n",
    "                           f'new {change_node}'])\n",
    "    changes.loc['dif'] = (changes.loc[f'new {change_node}'] - \n",
    "                          changes.loc[f'old {change_node}']\n",
    "                          )*100/changes.loc[f'old {change_node}']\n",
    "\n",
    "    a = changes.transpose()['dif'].plot.bar(rot=90)\n",
    "    plt.ylabel('Change (%)')\n",
    "    plt.xlabel('')\n",
    "    plt.xticks([0,1], ['not hired', 'hired'], rotation='horizontal')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = effect_of_changes('drive', 'hired', {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, ie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift of citric_acid to higher levels in the wine:\n",
    "_ = effect_of_changes('social_commitment', 'hired', {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, ie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like an intervention to increase education is a v good idea, if candidates are to become more likely to be hired, same for social comittment. The effect_of_change function allows for quick querying to determine the change that results from a shift in the underlying distributions.<br>\n",
    "<br>\n",
    "<br>\n",
    "Moreover, in addition to interventions like the above one, bayesian nets of course also allow to look at single cases. As an example, looking at candidate font:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cf = test_w[test_w['font']==0].iloc[0:1,:]\n",
    "cf2 = cf.copy()\n",
    "cf3 = cf.copy()\n",
    "\n",
    "cf2['font'] = 1\n",
    "cf3['font'] = 3\n",
    "\n",
    "predictions_cf = bn.predict_probability(cf,'hired')\n",
    "predictions_cf2 = bn.predict_probability(cf2,'hired')\n",
    "predictions_cf3 = bn.predict_probability(cf3,'hired')\n",
    "\n",
    "print(f'Changing the application font from Comic Sans (font=0) to a more appropriate font (font>=1), improves the chance of being hired for candidate {cf2.index[0]} from {predictions_cf.iloc[0,1]*100:.1f}% to about {predictions_cf2.iloc[0,1]*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit186273ec2ef84f499ca4d4874d7dfef8",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
